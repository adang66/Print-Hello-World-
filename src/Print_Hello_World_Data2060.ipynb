{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fep4EhP0Epl7"
   },
   "source": [
    "## **CART Regression Model**\n",
    "\n",
    "**Group Name** : Print (\"Hello World\")\n",
    "\n",
    "**Group Members** : Arpit Dang, Devraj Raghuvanshi, Matthew Dall'Asen, Varun Satheesh\n",
    "\n",
    "**GitHub Link**: https://github.com/adang66/Print-Hello-World-\n",
    "\n",
    "**Date**: December 15th, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PdTUdmQofFlG"
   },
   "source": [
    "## **1. Overview**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zogcwrn9Ep5k"
   },
   "source": [
    "The objective behind the Classification and Regression Tree (CART) algorithm is to iteratively select features within the feature space, and splitting it into a binary decsion, based on a determined split point, to predict the target variable. This process is repeated for each resulting subset until a stopping criterion is met, such as the depth of the tree, number of epochs and/or inability to further improve homogeneity. This report will leverage the regression tree of CART, where the objective is to predict the value of a new data point by taking the average of the target variable values in the leaf node corresponding to the data point.  \n",
    "\n",
    "Decision trees are widely used for machine learning problems due to their ability to handle non-linearity, making them well-suited for modeling complex relationships between features and the target variable. Additionally, decision trees inherently perform feature selection, focusing splits on relevant features and reducing the risk of overfitting to irrelevant variables. Finally, their interpretability provides a clear and easy-to-understand decision paths that explain their predictions.\n",
    "\n",
    "However, decisions trees are prone to overfitting, especially when the tree grows too deep, capturing noise in the data rather than meaningful patterns. Another limitation is their instability, as small changes in the training data can result in significantly different tree structures due to their greedy splitting approach. Moreover, decision trees can exhibit bias in their splitting criteria, often favoring features with more levels or categories, which might skew the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ogOV8FVeEdxe"
   },
   "source": [
    "## **2. Representation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HT8oVJMZl-9l"
   },
   "source": [
    "In CART (Classification and Regression Trees) for regression tasks, we define the input space as $\\mathscr{X} = \\mathbb{R}^d$, where each data point $\\mathbf{x_i} = [x_{i1}, x_{i2}, \\dots, x_{id}]$ consists of $d$ features. Each feature corresponds to a dimension in this $d$-dimensional space. The output space is defined as $\\mathscr{Y} = \\mathbb{R}$, indicating that the model predicts continuous values, which is suitable for regression.\n",
    "\n",
    "The regression tree itself is a binary decision tree. At each internal node (N), the tree partitions the input space based on a single feature, $j$, selected from among the $d$ features. This feature may vary at each node, depending on which split minimizes the prediction error best. To create a split, the algorithm selects a threshold value $t$ for the chosen feature $j$, effectively dividing the data into two groups based on whether the $j$-th feature value of each point is less than or equal to $t$ or greater than $t$. For any data point $\\mathbf{x}_i$, the tree assigns a direction based on the following condition:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6v-BRX2boyih"
   },
   "source": [
    "$$\n",
    "\\text{Direction}(\\mathbf{x}_{i}, N) =\n",
    "\\begin{cases}\n",
    "\\text{left child node}, & x_{ij} \\leq t \\\\\n",
    "\\text{right child node}, & x_{ij} > t\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlyBpKL8o3FS"
   },
   "source": [
    "\n",
    "This recursive partitioning of the input space continues until the data reaches a leaf node. Each leaf node, denoted $L_k$, represents a specific region in the feature space and corresponds to a unique subset of the training data that falls within that region. For any data point that reaches $L_k$, the prediction $\\hat{y}_k$ is typically calculated as the mean of the target values $y$ for all training observations that lie within that leaf. Specifically, if $n_k$ represents the number of observations in $L_k$, then:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EKo148To5CM"
   },
   "source": [
    "\n",
    "$$\n",
    "\\hat{y}_k = \\frac{1}{n_k} \\sum_{m \\in L_k} y_m\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEx8l-xVo6nG"
   },
   "source": [
    "\n",
    "Thus, each region defined by a leaf node provides a constant prediction that reflects the average target value of the training points within that region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oh7yH6kOcR8Q"
   },
   "source": [
    "## **3. Loss Function**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CbGJa3YycbLX"
   },
   "source": [
    "In contrast to the classification version of CART, which typically uses Gini impurity or entropy to measure the quality of splits, CART regression uses Mean of Squared Errors (MSE) as its loss function. MSE measures the total squared difference between the actual target values and the predicted values within each node and averages them out. Minimizing MSE allows the model to identify splits that reduce the overall variance in target values within each region, leading to more accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4kKRPRMGLBD"
   },
   "source": [
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOi2gkrkHgnY"
   },
   "source": [
    "where:\n",
    "\n",
    "- $n$ is the total number of observations\n",
    "- $y_i$ is the target value of the $i^{th}$ sample\n",
    "- $\\hat{y}_i$ is the predicted value for the $i^{th}$ sample given the learned model weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjTFM3tLpf2O"
   },
   "source": [
    "In CART regression, MSE is used at each node to evaluate and select the optimal feature and threshold for splitting. By choosing splits that minimize MSE, the model creates more homogenous regions with respect to the target variable, enhancing the accuracy of predictions in each region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hSBmZOH7Xmj4"
   },
   "source": [
    "## **4. Decision Tree Algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ok2HPj9YXwh1"
   },
   "source": [
    "At each internal node, CART regression evaluates potential splits across different features and threshold values to find the optimal partition that minimizes MSE. The process for determining the best split is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKddxLpzqhZN"
   },
   "source": [
    "#### 3.1 Splitting the Data into Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ys5VmuCaq8qc"
   },
   "source": [
    "For a given feature $ j $ and threshold $ t $, the data is divided into two groups:\n",
    "\n",
    "- **Group 1**: $ D_1 = \\{(\\mathbf{x}_i, y_i) \\mid x_{ij} \\leq t\\} $, containing all data points where the $ j $-th feature value is less than or equal to $ t $.\n",
    "- **Group 2**: $ D_2 = \\{(\\mathbf{x}_i, y_i) \\mid x_{ij} > t\\} $, containing all data points where the $ j $-th feature value is greater than $ t $.\n",
    "\n",
    "This division creates two subsets of data, each representing a distinct region in the feature space based on the chosen split.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8Et2C6Aq9Ce"
   },
   "source": [
    "#### 3.2 Calculating MSE for Each Group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCftDqmQq-9w"
   },
   "source": [
    "The **Mean of Squared Errors (MSE)** for each subset $D_1$ and $D_2$ is calculated to assess the quality of the split. This calculation reflects the variance of the target values within each group, which should ideally be minimized. The MSE for each group is given by:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5U14PQoAsKQq"
   },
   "source": [
    "$$\n",
    "\\text{MSE}(D_1) = \\frac{1}{n}\\sum_{(\\mathbf{x}_i, y_i) \\in D_1} (y_i - \\hat{y}_{D_1})^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{MSE}(D_2) = \\frac{1}{n}\\sum_{(\\mathbf{x}_i, y_i) \\in D_2} (y_i - \\hat{y}_{D_2})^2\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwnJb0oMsLqF"
   },
   "source": [
    "where:\n",
    "- $ |D_1| $ and $ |D_2| $ denote the number of observations in $ D_1 $ and $ D_2 $, respectively,\n",
    "- $ \\hat{y}_{D_1} $ and $ \\hat{y}_{D_2} $ represent the mean target values of observations in $ D_1 $ and $ D_2 $, respectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pIFgvErsVGs"
   },
   "source": [
    "By calculating the MSE for each group, the algorithm quantifies how closely the target values are clustered around their mean, indicating the “purity” of each split.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVdB-vOJHgpc"
   },
   "source": [
    "### 3.3 Calculating the Gain for the Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7K3QJJZmt-gt"
   },
   "source": [
    "The total gain for a split based on feature $j$ and threshold $t$ is derived from the decrease in impurity between the parent node and the resulting groups:\n",
    "\n",
    "$$\n",
    "\\text{Gain} = \\text{MSE}_{\\text{parent}} - (\\text{MSE}_{\\text{left child}} + \\text{MSE}_{\\text{right child}})\n",
    "$$\n",
    "\n",
    "This overall gain reflects the quality of the split: a higher gain indicates that the split has effectively reduced the variance in the target values within each resulting group, making it more likely to be selected as an optimal partition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNzjeArEHi9c"
   },
   "source": [
    "#### 3.4 Selecting the Best Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iXagfeMXuDRu"
   },
   "source": [
    "The algorithm evaluates all possible splits across features and thresholds and selects the split that maximizes the gain. Mathematically, the best split is determined as:\n",
    "\n",
    "$$\n",
    "\\text{Best Split} = \\arg\\max_{t,j} \\text{Gain}(t,j)\n",
    "$$\n",
    "\n",
    "This recursive process of selecting optimal splits continues, partitioning each node to maximize gain, until a stopping criterion is met. This approach ensures that each node split contributes to a more accurate representation of the target values within the tree’s regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcXrJVderFwo"
   },
   "source": [
    "#### 3.5. Stopping Criteria in CART Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LgYMS9wSrGCO"
   },
   "source": [
    "To avoid overfitting and control the complexity of the tree, CART regression employs specific stopping criteria. These criteria determine when the recursive partitioning should halt, ensuring the tree is both interpretable and generalizes well on unseen data. Common stopping criteria include:\n",
    "\n",
    "- Dataset is Empty: If the dataset is empty, the mean target value computed over the entire dataset is predicted.\n",
    "\n",
    "- Maximum Tree Depth: A predefined maximum depth limits the growth of the tree, preventing it from becoming overly complex. Once the maximum depth is reached, no further splits are performed, and nodes at this depth become leaf nodes.\n",
    "\n",
    "- Minimum Samples per Leaf: The tree stops splitting a node if the number of samples within that node is below a specified threshold. This criterion ensures that each leaf node contains a sufficient number of observations to produce stable predictions, reducing the likelihood of high variance and overfitting.\n",
    "\n",
    "- Homogeneous Target Values: If all the target values in a node are identical, no further splits are made. This condition ensures that the tree does not split unnecessarily when the target values are already perfectly predicted, making the node a leaf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2WUXSq_avjI-"
   },
   "source": [
    "## **5. Pseudocode for Decision Tree**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIWxpp6IJGkf"
   },
   "source": [
    "#### 4.1. Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vh-sAGbgJR_N"
   },
   "source": [
    "- **D**: Dataset containing $N$ samples, where each sample $\\mathbf{x_i} = [y_i, \\mathbf{f_i}]$ includes the target value $y_i$ and features $\\mathbf{f_i}$.  \n",
    "- **max\\_depth**: Maximum allowed depth of the tree.  \n",
    "- **min\\_samples\\_split**: Minimum number of samples required to split a node.  \n",
    "<!-- - (Optional) Validation dataset **V** for pruning. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gW-QSMtqJT16"
   },
   "source": [
    "#### 4.2. Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AzBnTMzhcbtQ"
   },
   "source": [
    "* Initialization  \n",
    "   - Compute the **mean target value** across the dataset $D$.  \n",
    "   - Initialize the **root node** $R$ with this mean value.\n",
    "\n",
    "* Recursive Splitting  \n",
    "   <!-- - Call the function `SplitRecursively(Node, Data, Indices)` starting from the root. -->\n",
    "   - Call the function `SplitRecursively(Node, Data)` starting from the root.\n",
    "\n",
    "   <!-- #### **Function**: `SplitRecursively(Node, Data, Indices)`   -->\n",
    "   - Function: `SplitRecursively(Node, Data)`  \n",
    "    - Terminal Condition  \n",
    "      - If one of the following holds:  \n",
    "        - $\\text{depth}(Node) \\geq \\text{max\\_depth}$  \n",
    "        - $\\text{number of samples} < \\text{min\\_samples\\_split}$  \n",
    "        - All target values $y_i$ in the current node are identical.  \n",
    "      - Set the node as a **leaf** and assign it the **mean target value** of the current data.\n",
    "\n",
    "    - Feature Selection  \n",
    "      <!-- - For each feature index $j \\in \\text{Indices}$:   -->\n",
    "      - For each feature index $j$:  \n",
    "        - Sort the data by feature $j$.  \n",
    "        - For each valid split point $i$, calculate the **split threshold**:  \n",
    "          $\\text{threshold} = \\frac{\\mathbf{f_i}[j] + \\mathbf{f_{i-1}}[j]}{2}$\n",
    "        - Compute the **gain** (decrease in impurity) from this split using:  \n",
    "          $\\text{Gain} = \\text{MSE}_\\text{before} - \\text{MSE}_\\text{after}$\n",
    "      - Select the feature $j^\\*$ and threshold with the **maximum gain**.\n",
    "\n",
    "    - Data Partitioning  \n",
    "      - Partition the data into:  \n",
    "        $D_\\text{left} = \\{ \\mathbf{x_i} \\mid \\mathbf{f_i}[j^\\*] \\leq \\text{threshold} \\}, \\quad D_{\\text{right}} = \\{ \\mathbf{x_i} \\mid \\mathbf{f_i}[j^\\*] > \\text{threshold} \\}$\n",
    "      - Create left and right child nodes.\n",
    "\n",
    "    - Recursion  \n",
    "      - Call `SplitRecursively` on the left and right child nodes with their respective datasets.\n",
    "\n",
    "<!-- **Variance Calculation**  \n",
    "   #### **Function**: `Variance(Values)`  \n",
    "   - Compute the variance of a list of values:  \n",
    "     $$\\text{Variance}(Y) = \\frac{1}{|Y|} \\sum_{i=1}^{|Y|} (y_i - \\hat{y})^2$$\n",
    "   - Where $\\hat{y}$ is the mean of the target values.\n",
    "\n",
    "**Pruning (Optional)**  \n",
    "   - If validation data $V$ is provided, call a recursive **pruning function** to simplify the tree by removing unnecessary splits.\n",
    "\n",
    "**Prediction**  \n",
    "   - For a given input $\\mathbf{f}$, traverse the tree from the root node based on feature values until a **leaf node** is reached.  \n",
    "   - Return the **mean target value** stored in the leaf node. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOCWjBKsJumy"
   },
   "source": [
    "#### 4.3. Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbGByCH3JvWH"
   },
   "source": [
    "- Trained Decision Tree model for regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQQpbxRFLm01"
   },
   "source": [
    "## **6. Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "nKaMRfk9Lmfl"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, left=None, right=None, depth=0, index_split_on=0, \n",
    "                 isleaf=False, pred=0.0, threshold=0):\n",
    "        \"\"\"\n",
    "        Initialize a Node for the CART tree.\n",
    "\n",
    "        Parameters:\n",
    "            left (Node): The left child node.\n",
    "            right (Node): The right child node.\n",
    "            depth (int): The depth of the node in the tree.\n",
    "            index_split_on (int): The feature index the node splits on.\n",
    "            isleaf (bool): Whether the node is a leaf.\n",
    "            pred (float): The predicted value for regression.\n",
    "            threshold (float): The threshold value for splitting.\n",
    "        \"\"\"\n",
    "\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.depth = depth\n",
    "        self.index_split_on = index_split_on\n",
    "        self.threshold = threshold\n",
    "        self.isleaf = isleaf\n",
    "        self.pred = pred  # Stores mean value for regression\n",
    "        self.info = {}\n",
    "\n",
    "\n",
    "    def _set_info(self, gain, num_samples):\n",
    "        \"\"\"\n",
    "        Helper function to set additional information about the node.\n",
    "\n",
    "        Parameters:\n",
    "            gain (float): The gain achieved at this node.\n",
    "            num_samples (int): The number of samples at this node.\n",
    "        \"\"\"\n",
    "\n",
    "        self.info['gain'] = gain\n",
    "        self.info['num_samples'] = num_samples\n",
    "\n",
    "\n",
    "class CART:\n",
    "\n",
    "    def __init__(self, data, min_samples_split=20, max_depth=5):\n",
    "        \"\"\"\n",
    "        Initialize the CART regression tree.\n",
    "\n",
    "        Parameters:\n",
    "            data (list of lists): The dataset where the first column \n",
    "            is the target value.\n",
    "            min_samples_split (int): Minimum number of samples required \n",
    "            to split a node.\n",
    "            max_depth (int): Maximum depth of the tree.\n",
    "        \"\"\"\n",
    "\n",
    "        y = [row[0] for row in data]\n",
    "        self.y_mean_dataset = np.mean(y)\n",
    "\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.root = Node(pred = self.y_mean_dataset)\n",
    "\n",
    "        indices = list(range(1, len(data[0])))\n",
    "\n",
    "        self._split_recurs(self.root, data, indices)\n",
    "\n",
    "\n",
    "    def predict(self, features):\n",
    "        \"\"\"\n",
    "        Predict the target value for a given set of features.\n",
    "\n",
    "        Parameters:\n",
    "            features (list): The feature values for prediction.\n",
    "\n",
    "        Returns:\n",
    "            float: Predicted target value.\n",
    "        \"\"\"\n",
    "\n",
    "        return self._predict_recurs(self.root, features)\n",
    "\n",
    "\n",
    "    def calculate_mae(self, data):\n",
    "        \"\"\"\n",
    "        Calculate Mean Absolute Error (MAE) on the given dataset.\n",
    "\n",
    "        Parameters:\n",
    "            data (list of lists): The dataset where the first column is \n",
    "            the target value.\n",
    "\n",
    "        Returns:\n",
    "            tuple: MAE and a list of predictions.\n",
    "        \"\"\"\n",
    "\n",
    "        absolute_error = 0.0\n",
    "        predictions = []\n",
    "        for row in data:\n",
    "            prediction = self.predict(row)\n",
    "            absolute_error += abs(prediction - row[0])  \n",
    "            # row[0] is the true target value\n",
    "            predictions.append(prediction)\n",
    "        mae = absolute_error / len(data)\n",
    "        return mae, predictions\n",
    "\n",
    "\n",
    "    def loss(self, data):\n",
    "        \"\"\"\n",
    "        Calculate Mean Squared Error (MSE) on the given dataset.\n",
    "\n",
    "        Parameters:\n",
    "            data (list of lists): The dataset where the first column \n",
    "            is the target value.\n",
    "\n",
    "        Returns:\n",
    "            tuple: MSE and a list of predictions.\n",
    "        \"\"\"\n",
    "\n",
    "        squared_error = 0.0\n",
    "        predictions = []\n",
    "        for row in data:\n",
    "            prediction = self.predict(row)\n",
    "            squared_error += (prediction - row[0]) ** 2  # row[0] is the true target value\n",
    "            predictions.append(prediction)\n",
    "        return squared_error / len(data), predictions\n",
    "\n",
    "\n",
    "    def _predict_recurs(self, node, row):\n",
    "        \"\"\"\n",
    "        Recursive helper function to predict the target value \n",
    "        for a row of data.\n",
    "\n",
    "        Parameters:\n",
    "            node (Node): The current node being traversed.\n",
    "            row (list): The feature values of the row.\n",
    "\n",
    "        Returns:\n",
    "            float: Predicted target value.\n",
    "        \"\"\"\n",
    "\n",
    "        if node.isleaf or node.index_split_on == 0:\n",
    "            return node.pred\n",
    "        if row[node.index_split_on] <= node.threshold:\n",
    "            return self._predict_recurs(node.left, row)\n",
    "        else:\n",
    "            return self._predict_recurs(node.right, row)\n",
    "\n",
    "\n",
    "    def _is_terminal(self, node, data, indices):\n",
    "        \"\"\"\n",
    "        Check if a node should be a terminal node.\n",
    "\n",
    "        Parameters:\n",
    "            node (Node): The current node being evaluated.\n",
    "            data (list of lists): The data at this node.\n",
    "            indices (list): List of feature indices.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (bool indicating if terminal, \n",
    "            predicted value if terminal).\n",
    "        \"\"\"\n",
    "\n",
    "        y = [row[0] for row in data]\n",
    "\n",
    "        # If the dataset is empty\n",
    "        if len(data) == 0:\n",
    "            return True, self.y_mean_dataset  # Default value\n",
    "\n",
    "        # If all target values are the same\n",
    "        if len(set(y)) == 1:\n",
    "            return True, y[0]  # All values are identical\n",
    "\n",
    "        # If maximum depth is reached\n",
    "        if node.depth >= self.max_depth:\n",
    "            return True, sum(y) / len(y)  # Mean value\n",
    "\n",
    "        # If the number of samples is less than min_samples_split\n",
    "        if len(data) < self.min_samples_split:\n",
    "            return True, sum(y) / len(y)  # Mean value\n",
    "\n",
    "        return False, sum(y) / len(y)  # Continue splitting\n",
    "\n",
    "\n",
    "    def _split_recurs(self, node, data, indices):\n",
    "        \"\"\"\n",
    "        Recursive function to split the data and create child nodes.\n",
    "\n",
    "        Parameters:\n",
    "            node (Node): The current node being processed.\n",
    "            data (list of lists): The data at this node.\n",
    "            indices (list): List of feature indices to consider for \n",
    "            splitting.\n",
    "        \"\"\"\n",
    "\n",
    "        # Check if the current node is terminal\n",
    "        node.isleaf, node.pred = self._is_terminal(node, data, indices)\n",
    "\n",
    "        if not node.isleaf:\n",
    "            max_gain = -float('inf')\n",
    "            best_index = -1\n",
    "\n",
    "            # Iterate through each feature\n",
    "            for index in indices:\n",
    "                # Sort data based on the current feature\n",
    "                sorted_data = sorted(data, key=lambda x: x[index])\n",
    "                for i in range(1, len(sorted_data)):\n",
    "                    if sorted_data[i][index] != sorted_data[i-1][index]:\n",
    "                        threshold = (sorted_data[i][index] + \n",
    "                                     sorted_data[i-1][index]) / 2\n",
    "                        gain = self._calc_gain(data, index, threshold)\n",
    "                        if gain > max_gain:\n",
    "                            max_gain = gain\n",
    "                            best_index = index\n",
    "                            best_threshold = threshold\n",
    "\n",
    "            if best_index == -1:\n",
    "                node.isleaf = True\n",
    "                node.pred = sum(row[0] for row in data) / len(data)\n",
    "                return\n",
    "\n",
    "            # Store the best index and threshold\n",
    "            node.index_split_on = best_index\n",
    "            node.threshold = best_threshold\n",
    "            node._set_info(max_gain, len(data))\n",
    "\n",
    "            # Split the data\n",
    "            left_data = [row for row in data \n",
    "                         if row[best_index] <= best_threshold]\n",
    "            right_data = [row for row in data \n",
    "                          if row[best_index] > best_threshold]\n",
    "\n",
    "            # Create child nodes\n",
    "            node.left = Node(depth=node.depth + 1)\n",
    "            node.right = Node(depth=node.depth + 1)\n",
    "\n",
    "            # Recur on the left and right nodes\n",
    "            self._split_recurs(node.left, left_data, indices)\n",
    "            self._split_recurs(node.right, right_data, indices)\n",
    "\n",
    "\n",
    "    def _calc_gain(self, data, split_index, threshold):\n",
    "        \"\"\"\n",
    "        Calculate the variance reduction (gain) for a given split.\n",
    "\n",
    "        Parameters:\n",
    "            data (list of lists): The data to evaluate the split.\n",
    "            split_index (int): The index of the feature to split on.\n",
    "            threshold (float): The threshold value for splitting.\n",
    "\n",
    "        Returns:\n",
    "            float: The variance reduction achieved by the split.\n",
    "        \"\"\"\n",
    "\n",
    "        y = [row[0] for row in data]\n",
    "        xi = [row[split_index] for row in data]\n",
    "\n",
    "        if len(y) == 0:\n",
    "            return 0\n",
    "\n",
    "        # Variance before the split\n",
    "        variance_before = self._variance(y)\n",
    "\n",
    "        # Split data by feature value\n",
    "        left_y = [y[i] for i in range(len(y)) if xi[i] <= threshold]\n",
    "        right_y = [y[i] for i in range(len(y)) if xi[i] > threshold]\n",
    "\n",
    "        # Variance after the split\n",
    "        left_variance = self._variance(left_y) if len(left_y) > 0 else 0\n",
    "        right_variance = self._variance(right_y) if len(right_y) > 0 else 0\n",
    "\n",
    "        variance_after = (len(left_y) / len(y)) * left_variance + (len(right_y) / len(y)) * right_variance   # WEIGHTED VARIANCE REDUCTION\n",
    "        # variance_after = left_variance + right_variance\n",
    "\n",
    "        gain = variance_before - variance_after\n",
    "        return gain\n",
    "\n",
    "\n",
    "    def _variance(self, values):\n",
    "        \"\"\"\n",
    "        Compute the variance of a list of values.\n",
    "\n",
    "        Parameters:\n",
    "            values (list): Numerical values for which variance \n",
    "            is to be calculated.\n",
    "\n",
    "        Returns:\n",
    "            float: Variance of the input values.\n",
    "        \"\"\"\n",
    "        mean_value = sum(values) / len(values)\n",
    "        return sum((x - mean_value) ** 2 for x in values) / len(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fWJYd-CLrhh"
   },
   "source": [
    "## **7. Unit Tests**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kgPJ4YjiMHOo"
   },
   "source": [
    "#### Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "mNNQlmMB1TdL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests pass successfully!\n"
     ]
    }
   ],
   "source": [
    "import pytest\n",
    "# Sets random seed for testing purposes\n",
    "np.random.seed(0)\n",
    "\n",
    "# Create test data\n",
    "data1 = [\n",
    "    [2.5, 0, 1, 0],\n",
    "    [1.5, 1, 0, 1],\n",
    "    [3.0, 0, 1, 1],\n",
    "    [2.0, 1, 0, 0],\n",
    "    [3.5, 0, 1, 1],\n",
    "]\n",
    "\n",
    "data2 = [\n",
    "    [1.0, 0, 0, 0],\n",
    "    [2.0, 1, 1, 0],\n",
    "    [3.0, 1, 0, 1],\n",
    "    [4.0, 0, 1, 1],\n",
    "    [5.0, 1, 1, 1],\n",
    "    [6.0, 0, 0, 0],\n",
    "]\n",
    "\n",
    "# Test Models\n",
    "test_model1 = CART(data1, min_samples_split=2, max_depth=3)\n",
    "test_model2 = CART(data2, min_samples_split=2, max_depth=3)\n",
    "\n",
    "# Test model initialization\n",
    "assert isinstance(test_model1, CART)\n",
    "assert isinstance(test_model2, CART)\n",
    "assert test_model1.min_samples_split == 2\n",
    "assert test_model1.max_depth == 3\n",
    "assert test_model2.min_samples_split == 2\n",
    "assert test_model2.max_depth == 3\n",
    "\n",
    "# Test root node properties\n",
    "assert isinstance(test_model1.root, Node)\n",
    "assert isinstance(test_model2.root, Node)\n",
    "assert test_model1.root.depth == 0\n",
    "assert test_model2.root.depth == 0\n",
    "\n",
    "# Test prediction functionality\n",
    "pred1 = test_model1.predict([0, 1, 0, 1])\n",
    "pred2 = test_model2.predict([0, 1, 1, 1])\n",
    "\n",
    "assert isinstance(pred1, (int, float))\n",
    "assert isinstance(pred2, (int, float))\n",
    "\n",
    "# Test Mean Absolute Error (MAE) calculation\n",
    "mae1, predictions1 = test_model1.calculate_mae(data1)\n",
    "mae2, predictions2 = test_model2.calculate_mae(data2)\n",
    "\n",
    "assert isinstance(mae1, float)\n",
    "assert isinstance(mae2, float)\n",
    "assert len(predictions1) == len(data1)\n",
    "assert len(predictions2) == len(data2)\n",
    "\n",
    "# Test Mean Squared Error (MSE) calculation\n",
    "mse1, predictions1 = test_model1.loss(data1)\n",
    "mse2, predictions2 = test_model2.loss(data2)\n",
    "\n",
    "\n",
    "assert isinstance(mse1, float)\n",
    "assert isinstance(mse2, float)\n",
    "assert len(predictions1) == len(data1)\n",
    "assert len(predictions2) == len(data2)\n",
    "\n",
    "# Function to recursively check node structure\n",
    "def check_node(node):\n",
    "    if node.isleaf:\n",
    "        assert node.left is None and node.right is None\n",
    "    else:\n",
    "        assert isinstance(node.left, Node)\n",
    "        assert isinstance(node.right, Node)\n",
    "        assert node.left.depth == node.depth + 1\n",
    "        assert node.right.depth == node.depth + 1\n",
    "        check_node(node.left)\n",
    "        check_node(node.right)\n",
    "\n",
    "# Test tree structure\n",
    "check_node(test_model1.root)\n",
    "check_node(test_model2.root)\n",
    "\n",
    "# Function to recursively check split information\n",
    "def check_split_info(node):\n",
    "    if not node.isleaf:\n",
    "        assert 'gain' in node.info\n",
    "        assert 'num_samples' in node.info\n",
    "        assert isinstance(node.info['gain'], float)\n",
    "        assert isinstance(node.info['num_samples'], int)\n",
    "        check_split_info(node.left)\n",
    "        check_split_info(node.right)\n",
    "\n",
    "# Test split information\n",
    "check_split_info(test_model1.root)\n",
    "check_split_info(test_model2.root)\n",
    "\n",
    "# Test variance calculation\n",
    "values = [1, 2, 3, 4, 5]\n",
    "variance = test_model1._variance(values)\n",
    "assert isinstance(variance, float)\n",
    "assert np.isclose(variance, 2.0)\n",
    "\n",
    "# Test gain calculation\n",
    "data = [[1, 0], [2, 1], [3, 0], [4, 1], [5, 0]]\n",
    "gain = test_model1._calc_gain(data, 1, 0.5)\n",
    "assert isinstance(gain, float)\n",
    "assert gain >= 0\n",
    "\n",
    "print(\"All tests pass successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEzF7epRML7e"
   },
   "source": [
    "#### Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VNO3Anms_r08",
    "outputId": "8368447d-ae05-4f01-d94d-a2311e0a80fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on validation data: [3, 3, 3, 3]\n",
      "MSE Loss on validation data: 0.0\n",
      "All assertions passed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Define the training data and validation data\n",
    "data = [\n",
    "    [6, 10.5, 3],  # Target is in the first column\n",
    "    [4, 8.3, 2],\n",
    "    [7, 12.4, 5],\n",
    "    [5, 9.1, 3],\n",
    "    [3, 7.2, 1]\n",
    "]\n",
    "\n",
    "validation_data = [\n",
    "    [3, 2.5, 6],  # Expected: approximately 3\n",
    "    [3, 4.0, 3],  # Expected: approximately 3\n",
    "    [3, 5.5, 8],  # Expected: approximately 3\n",
    "    [3, 6.0, 4]   # Expected: approximately 3\n",
    "]\n",
    "\n",
    "# Helper function to check approximate equality\n",
    "def approx(actual, expected, tolerance=0.1):\n",
    "    \"\"\"Check if the actual value is within the expected \n",
    "    value +/- tolerance.\"\"\"\n",
    "    return abs(actual - expected) <= tolerance\n",
    "\n",
    "# Initialize your CART model\n",
    "cart_model = CART(data, min_samples_split=2, max_depth=3)\n",
    "\n",
    "# Calculate loss and predictions\n",
    "loss, predictions = cart_model.loss(validation_data)\n",
    "\n",
    "# Display results\n",
    "print(\"Predictions on validation data:\", predictions)\n",
    "print(\"MSE Loss on validation data:\", loss)\n",
    "\n",
    "# Expected predictions for validation data\n",
    "expected_predictions = [3.0, 3.0, 3.0, 3.0]\n",
    "\n",
    "# Validate predictions\n",
    "assert len(predictions) == len(validation_data), \"Prediction length mismatch.\"\n",
    "# \"Prediction length mismatch.\"\n",
    "\n",
    "# Validate predictions against expected values\n",
    "for i, (pred, expected) in enumerate(zip(predictions, expected_predictions)):\n",
    "    assert approx(pred, expected), f\"Prediction {pred} is not approx {expected} for validation data index {i}.\"\n",
    "\n",
    "# Manually calculate expected MSE\n",
    "true_targets = [row[0] for row in validation_data]\n",
    "expected_mse = np.mean([(pred - true) ** 2 for pred, \n",
    "                        true in zip(expected_predictions, true_targets)])\n",
    "\n",
    "# Validate MSE\n",
    "assert approx(loss, expected_mse), f\"MSE {loss} is not approx {expected_mse}.\"\n",
    "\n",
    "print(\"All assertions passed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CuzvGDmNMMMO",
    "outputId": "c75cc0f4-ca0c-40f8-c4ba-6e9d82606563"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on validation data: [2, 2, 5, 2]\n",
      "MSE Loss on validation data: 0.5\n",
      "All assertions passed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Define the test data and validation data\n",
    "data = [\n",
    "    [4, 7.2, 1],  # Target is in the first column\n",
    "    [3, 6.8, 2],\n",
    "    [2, 5.5, 3],\n",
    "    [1, 3.3, 4],\n",
    "    [5, 8.1, 5]\n",
    "]\n",
    "\n",
    "validation_data = [\n",
    "    [3, 6.0, 3],  # Expected: approximately 2\n",
    "    [1, 4.5, 1],  # Expected: approximately 2\n",
    "    [5, 8.0, 5],  # Expected: approximately 5\n",
    "    [2, 5.5, 2]   # Expected: approximately 2\n",
    "]\n",
    "\n",
    "# Helper function to check approximate equality\n",
    "def approx(actual, expected, tolerance=0.1):\n",
    "    \"\"\"Check if the actual value is within the \n",
    "    expected value +/- tolerance.\"\"\"\n",
    "    return abs(actual - expected) <= tolerance\n",
    "\n",
    "# Initialize your CART model\n",
    "cart_model = CART(data, min_samples_split=2, max_depth=3)\n",
    "\n",
    "# Calculate loss and predictions\n",
    "loss, predictions = cart_model.loss(validation_data)\n",
    "\n",
    "# Display results\n",
    "print(\"Predictions on validation data:\", predictions)\n",
    "print(\"MSE Loss on validation data:\", loss)\n",
    "\n",
    "# Expected predictions for validation data\n",
    "expected_predictions = [2.0, 2.0, 5.0, 2.0]\n",
    "\n",
    "# Validate predictions\n",
    "assert len(predictions) == len(validation_data), \"Prediction length mismatch.\"\n",
    "assert approx(predictions[0], 2.0), f\"Prediction {predictions[0]} is not approx 2.0.\"\n",
    "assert approx(predictions[1], 2.0), f\"Prediction {predictions[1]} is not approx 2.0.\"\n",
    "assert approx(predictions[2], 5.0), f\"Prediction {predictions[2]} is not approx 5.0.\"\n",
    "assert approx(predictions[3], 2.0), f\"Prediction {predictions[3]} is not approx 2.0.\"\n",
    "\n",
    "\n",
    "# Manually calculate expected MSE\n",
    "expected_mse = np.mean([(pred - true[0]) ** 2 for pred, \n",
    "                        true in zip(expected_predictions, validation_data)])\n",
    "\n",
    "# Validate MSE\n",
    "assert approx(loss, expected_mse), f\"MSE {loss} is not approx {expected_mse}.\"\n",
    "\n",
    "print(\"All assertions passed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 4 (Edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty dataset test passed: list index out of range\n",
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "\n",
    "# Empty dataset test\n",
    "empty_data = []\n",
    "try:\n",
    "    cart = CART(empty_data)\n",
    "except IndexError as e:\n",
    "    print(\"Empty dataset test passed:\", str(e))\n",
    "\n",
    "# Single data point test\n",
    "single_point_data = [[5.0, 1.2, 3.4]]\n",
    "cart = CART(single_point_data, min_samples_split=2, max_depth=3)\n",
    "prediction = cart.predict([1.2, 3.4])\n",
    "assert np.isclose(prediction, 5.0), \"Single data point test failed\"\n",
    "\n",
    "# Identical features test\n",
    "identical_features = [[3.0, 2.0, 2.0], [3.0, 2.0, 2.0], [3.0, 2.0, 2.0]]\n",
    "cart = CART(identical_features, min_samples_split=2, max_depth=3)\n",
    "prediction = cart.predict([2.0, 2.0])\n",
    "assert np.isclose(prediction, 3.0), \"Identical features test failed\"\n",
    "\n",
    "# No variance in target test\n",
    "no_variance_target = [[1.0, 3.4, 5.6], [1.0, 2.1, 4.3], [1.0, 3.0, 3.3]]\n",
    "cart = CART(no_variance_target, min_samples_split=2, max_depth=3)\n",
    "prediction = cart.predict([3.4, 5.6])\n",
    "assert np.isclose(prediction, 1.0), \"No variance in target test failed\"\n",
    "\n",
    "# Min samples split not met test\n",
    "data = [[5.0, 1.0], [10.0, 2.0]]\n",
    "cart = CART(data, min_samples_split=5, max_depth=3)\n",
    "assert cart.root.isleaf, \"Min samples split not met test failed\"\n",
    "\n",
    "# High variance split test\n",
    "data = [[1.0, 2.0], [100.0, 3.0], [1000.0, 4.0]]\n",
    "cart = CART(data, min_samples_split=2, max_depth=3)\n",
    "assert cart.root.threshold is not None, \"High variance split test failed\"\n",
    "\n",
    "# Predict with unseen features test\n",
    "data = [[5.0, 1.0], [10.0, 2.0], [15.0, 3.0]]\n",
    "cart = CART(data, min_samples_split=1, max_depth=3)\n",
    "prediction = cart.predict([100.0, 4.0])\n",
    "assert prediction is not None, \"Predict with unseen features test failed\"\n",
    "\n",
    "print(\"All tests passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEzT3jzBLvSA"
   },
   "source": [
    "## **8. Main**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QigSrerh7tSG"
   },
   "source": [
    "#### California Housing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8_TCbsy_Lvh8",
    "outputId": "a54b1de9-841b-471c-b138-fa971fcb5c88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn MSE: 0.5245\n",
      "Custom CART MSE: 0.5245\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the dataset\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Format the data for the custom CART implementation (y comes first)\n",
    "train_data = np.hstack((y_train.reshape(-1, 1), X_train))\n",
    "test_data = np.hstack((y_test.reshape(-1, 1), X_test))\n",
    "\n",
    "# Train sklearn's DecisionTreeRegressor\n",
    "sklearn_model = DecisionTreeRegressor(max_depth=5, min_samples_split=20)\n",
    "sklearn_model.fit(X_train, y_train)\n",
    "sklearn_predictions = sklearn_model.predict(X_test)\n",
    "sklearn_mse = mean_squared_error(y_test, sklearn_predictions)\n",
    "\n",
    "# Train your CART implementation\n",
    "custom_model = CART(data=train_data, max_depth=5, min_samples_split=20)\n",
    "custom_mse, custom_predictions = custom_model.loss(test_data)\n",
    "\n",
    "# Compare results\n",
    "print(f\"Sklearn MSE: {sklearn_mse:.4f}\")\n",
    "print(f\"Custom CART MSE: {custom_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0e__q1Sm8V9S"
   },
   "source": [
    "#### Diabetes Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "itZYRXMC8XJU",
    "outputId": "7c23be8f-8c62-4671-8ede-12292406e90a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn MSE: 3358.6384\n",
      "Custom CART MSE: 3378.0334\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the dataset\n",
    "data = load_diabetes()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Format the data for the custom CART implementation (y comes first)\n",
    "train_data = np.hstack((y_train.reshape(-1, 1), X_train))\n",
    "test_data = np.hstack((y_test.reshape(-1, 1), X_test))\n",
    "\n",
    "# Train sklearn's DecisionTreeRegressor\n",
    "sklearn_model = DecisionTreeRegressor(max_depth=5, min_samples_split=20, random_state=42)\n",
    "sklearn_model.fit(X_train, y_train)\n",
    "sklearn_predictions = sklearn_model.predict(X_test)\n",
    "sklearn_mse = mean_squared_error(y_test, sklearn_predictions)\n",
    "\n",
    "# Train your CART implementation\n",
    "custom_model = CART(data=train_data, max_depth=5, min_samples_split=20)\n",
    "custom_mse, custom_predictions = custom_model.loss(test_data)\n",
    "\n",
    "# Compare results\n",
    "print(f\"Sklearn MSE: {sklearn_mse:.4f}\")\n",
    "print(f\"Custom CART MSE: {custom_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kW1STiVz6mXF"
   },
   "source": [
    "**Previous Work:** [A Data Mining Approach to Predict Forest Fires using Meteorological Data](https://core.ac.uk/download/pdf/55609027.pdf)\n",
    "\n",
    "In their paper \"A Data Mining Approach to Predict Forest Fires using Meteorological Data,\" Cortez et al. (2007) explored the use of machine learning algorithms to predict the area burned in forest fires using meteorological data. They tested several ML techniques, including CART. The study demonstrated that meteorological features such as temperature, wind, relative humidity, and rainfall significantly contribute to predictive performance. The dataset used was the [Forest Fires dataset](https://https://archive.ics.uci.edu/dataset/162/forest+fires) (available on UCI ML Repository), consisting of 517 samples and 12 features.\n",
    "\n",
    "**Evaluation Metric:** Mean Absolute Error (MAE)\n",
    "\n",
    "We replicate the methodology outlined in the paper using the following steps:\n",
    "\n",
    "- Preprocess the data (e.g., one-hot encoding and standardization).\n",
    "- Use Decision Tree with CART implementation.\n",
    "- Perform 30 rounds of 10-fold cross-validation.\n",
    "- Evaluate performance using MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pkCsy3WZUDnL",
    "outputId": "35ceadc8-b44a-4e2a-b217-7d4bdb2fd6cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ucimlrepo\n",
      "  Downloading ucimlrepo-0.0.7-py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ucimlrepo) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.10/dist-packages (from ucimlrepo) (2024.8.30)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.16.0)\n",
      "Downloading ucimlrepo-0.0.7-py3-none-any.whl (8.0 kB)\n",
      "Installing collected packages: ucimlrepo\n",
      "Successfully installed ucimlrepo-0.0.7\n"
     ]
    }
   ],
   "source": [
    "!pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oibeRsOfT_AD"
   },
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# fetch dataset\n",
    "forest_fires = fetch_ucirepo(id=162)\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "X = forest_fires.data.features\n",
    "y = forest_fires.data.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "JdJlH7iZUTm2",
    "outputId": "f79f4138-2bd5-47b6-c3df-58c1e40b1255"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"X\",\n  \"rows\": 517,\n  \"fields\": [\n    {\n      \"column\": \"X\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 1,\n        \"max\": 9,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          1,\n          8,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Y\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 2,\n        \"max\": 9,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          5,\n          4,\n          9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"month\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"may\",\n          \"dec\",\n          \"mar\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"day\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"fri\",\n          \"tue\",\n          \"wed\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"FFMC\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.5201108488512665,\n        \"min\": 18.7,\n        \"max\": 96.2,\n        \"num_unique_values\": 106,\n        \"samples\": [\n          95.9,\n          90.9,\n          92.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"DMC\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 64.0464822492543,\n        \"min\": 1.1,\n        \"max\": 291.3,\n        \"num_unique_values\": 215,\n        \"samples\": [\n          121.7,\n          166.9,\n          130.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"DC\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 248.06619170584375,\n        \"min\": 7.9,\n        \"max\": 860.6,\n        \"num_unique_values\": 219,\n        \"samples\": [\n          458.8,\n          700.7,\n          665.6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ISI\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.5594771752160375,\n        \"min\": 0.0,\n        \"max\": 56.1,\n        \"num_unique_values\": 119,\n        \"samples\": [\n          7.9,\n          18.0,\n          14.7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"temp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.806625349573503,\n        \"min\": 2.2,\n        \"max\": 33.3,\n        \"num_unique_values\": 192,\n        \"samples\": [\n          23.0,\n          4.8,\n          11.6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RH\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 16,\n        \"min\": 15,\n        \"max\": 100,\n        \"num_unique_values\": 75,\n        \"samples\": [\n          29,\n          61,\n          72\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"wind\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.7916526009464604,\n        \"min\": 0.4,\n        \"max\": 9.4,\n        \"num_unique_values\": 21,\n        \"samples\": [\n          6.7,\n          8.5,\n          6.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rain\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.29595912089089405,\n        \"min\": 0.0,\n        \"max\": 6.4,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          0.0,\n          0.2,\n          0.4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "X"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-cad62d1a-0584-4cd9-ade3-affd654ec5b8\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>FFMC</th>\n",
       "      <th>DMC</th>\n",
       "      <th>DC</th>\n",
       "      <th>ISI</th>\n",
       "      <th>temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>wind</th>\n",
       "      <th>rain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>mar</td>\n",
       "      <td>fri</td>\n",
       "      <td>86.2</td>\n",
       "      <td>26.2</td>\n",
       "      <td>94.3</td>\n",
       "      <td>5.1</td>\n",
       "      <td>8.2</td>\n",
       "      <td>51</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>oct</td>\n",
       "      <td>tue</td>\n",
       "      <td>90.6</td>\n",
       "      <td>35.4</td>\n",
       "      <td>669.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>18.0</td>\n",
       "      <td>33</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>oct</td>\n",
       "      <td>sat</td>\n",
       "      <td>90.6</td>\n",
       "      <td>43.7</td>\n",
       "      <td>686.9</td>\n",
       "      <td>6.7</td>\n",
       "      <td>14.6</td>\n",
       "      <td>33</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>mar</td>\n",
       "      <td>fri</td>\n",
       "      <td>91.7</td>\n",
       "      <td>33.3</td>\n",
       "      <td>77.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>97</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>mar</td>\n",
       "      <td>sun</td>\n",
       "      <td>89.3</td>\n",
       "      <td>51.3</td>\n",
       "      <td>102.2</td>\n",
       "      <td>9.6</td>\n",
       "      <td>11.4</td>\n",
       "      <td>99</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>aug</td>\n",
       "      <td>sun</td>\n",
       "      <td>81.6</td>\n",
       "      <td>56.7</td>\n",
       "      <td>665.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>27.8</td>\n",
       "      <td>32</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>aug</td>\n",
       "      <td>sun</td>\n",
       "      <td>81.6</td>\n",
       "      <td>56.7</td>\n",
       "      <td>665.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>21.9</td>\n",
       "      <td>71</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>aug</td>\n",
       "      <td>sun</td>\n",
       "      <td>81.6</td>\n",
       "      <td>56.7</td>\n",
       "      <td>665.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>21.2</td>\n",
       "      <td>70</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>aug</td>\n",
       "      <td>sat</td>\n",
       "      <td>94.4</td>\n",
       "      <td>146.0</td>\n",
       "      <td>614.7</td>\n",
       "      <td>11.3</td>\n",
       "      <td>25.6</td>\n",
       "      <td>42</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>nov</td>\n",
       "      <td>tue</td>\n",
       "      <td>79.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>106.7</td>\n",
       "      <td>1.1</td>\n",
       "      <td>11.8</td>\n",
       "      <td>31</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>517 rows × 12 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cad62d1a-0584-4cd9-ade3-affd654ec5b8')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-cad62d1a-0584-4cd9-ade3-affd654ec5b8 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-cad62d1a-0584-4cd9-ade3-affd654ec5b8');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-473370dd-7555-4045-a58d-97e2885d8918\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-473370dd-7555-4045-a58d-97e2885d8918')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-473370dd-7555-4045-a58d-97e2885d8918 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "  <div id=\"id_56bb4459-8b29-4410-9408-351da5ac7256\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('X')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_56bb4459-8b29-4410-9408-351da5ac7256 button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('X');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "     X  Y month  day  FFMC    DMC     DC   ISI  temp  RH  wind  rain\n",
       "0    7  5   mar  fri  86.2   26.2   94.3   5.1   8.2  51   6.7   0.0\n",
       "1    7  4   oct  tue  90.6   35.4  669.1   6.7  18.0  33   0.9   0.0\n",
       "2    7  4   oct  sat  90.6   43.7  686.9   6.7  14.6  33   1.3   0.0\n",
       "3    8  6   mar  fri  91.7   33.3   77.5   9.0   8.3  97   4.0   0.2\n",
       "4    8  6   mar  sun  89.3   51.3  102.2   9.6  11.4  99   1.8   0.0\n",
       "..  .. ..   ...  ...   ...    ...    ...   ...   ...  ..   ...   ...\n",
       "512  4  3   aug  sun  81.6   56.7  665.6   1.9  27.8  32   2.7   0.0\n",
       "513  2  4   aug  sun  81.6   56.7  665.6   1.9  21.9  71   5.8   0.0\n",
       "514  7  4   aug  sun  81.6   56.7  665.6   1.9  21.2  70   6.7   0.0\n",
       "515  1  4   aug  sat  94.4  146.0  614.7  11.3  25.6  42   4.0   0.0\n",
       "516  6  3   nov  tue  79.5    3.0  106.7   1.1  11.8  31   4.5   0.0\n",
       "\n",
       "[517 rows x 12 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_vtULYctUjZC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqQ7Avmcvssl"
   },
   "source": [
    "#### Testing Agasint Sklearn's Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-bvMNzhvevzU"
   },
   "source": [
    "##### Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s7a_nbetfBKH",
    "outputId": "53c643da-8b07-4f92-ce0c-4c7b5b24de86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 13.609762745243872 +/- 0.37620042307863827\n"
     ]
    }
   ],
   "source": [
    "X_encoded = pd.get_dummies(X, columns=[\"month\", \"day\"])\n",
    "y_transformed = np.log1p(y)  # ln(x+1) transformation\n",
    "\n",
    "mae_avg = []\n",
    "\n",
    "for i in range(30):\n",
    "\n",
    "  mae_values = []\n",
    "\n",
    "  kf = KFold(n_splits=10, shuffle=True, random_state=i*42)\n",
    "\n",
    "  for i, (train_index, test_index) in enumerate(kf.split(X_encoded)):\n",
    "      X_train, X_test = X_encoded.iloc[train_index], X_encoded.iloc[test_index]\n",
    "      y_train, y_test = y_transformed.iloc[train_index], y_transformed.iloc[test_index]\n",
    "\n",
    "      scaler = StandardScaler()\n",
    "      X_train = scaler.fit_transform(X_train)\n",
    "      X_test = scaler.transform(X_test)\n",
    "\n",
    "      sklearn_model = DecisionTreeRegressor(max_depth=5, min_samples_split=42)\n",
    "      sklearn_model.fit(X_train, y_train)\n",
    "\n",
    "      sklearn_predictions = sklearn_model.predict(X_test)\n",
    "\n",
    "      y_pred = np.expm1(sklearn_predictions)\n",
    "      y_true = np.expm1(y_test)\n",
    "\n",
    "      sklearn_mae = mean_absolute_error(y_true, y_pred)\n",
    "      mae_values.append(sklearn_mae)\n",
    "\n",
    "  average_mae = np.mean(mae_values)\n",
    "  mae_avg.append(average_mae)\n",
    "\n",
    "print(f\"MAE: {np.mean(mae_avg)} +/- {np.std(mae_avg)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "POVl4EZavx2R"
   },
   "source": [
    "##### Custom Cart Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KZVUaJiFhIDi",
    "outputId": "4a9aa5de-1c53-4934-d3a7-290447b413a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 13.609079338058589 +/- 0.3774537599572436\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "X_encoded = pd.get_dummies(X, columns=[\"month\", \"day\"])\n",
    "y_transformed = np.log1p(y)  # ln(x+1) transformation\n",
    "\n",
    "mae_avg = []\n",
    "\n",
    "for i in range(30):\n",
    "\n",
    "  mae_values = []\n",
    "\n",
    "  kf = KFold(n_splits=10, shuffle=True, random_state=i*42)\n",
    "\n",
    "  for i, (train_index, test_index) in enumerate(kf.split(X_encoded)):\n",
    "      X_train, X_test = X_encoded.iloc[train_index], X_encoded.iloc[test_index]\n",
    "      y_train, y_test = y_transformed.iloc[train_index], y_transformed.iloc[test_index]\n",
    "\n",
    "      X_train = np.array(X_train)\n",
    "      X_test = np.array(X_test)\n",
    "      y_train = np.array(y_train)\n",
    "      y_test = np.array(y_test)\n",
    "\n",
    "      scaler = StandardScaler()\n",
    "      X_train = scaler.fit_transform(X_train)\n",
    "      X_test = scaler.transform(X_test)\n",
    "\n",
    "      # Format the data for the custom CART implementation (y comes first)\n",
    "      train_data = np.hstack((y_train.reshape(-1, 1), X_train))\n",
    "      test_data = np.hstack((y_test.reshape(-1, 1), X_test))\n",
    "\n",
    "      # Train the CART implementation\n",
    "      custom_model = CART(data=train_data, max_depth=5, min_samples_split=42)\n",
    "      custom_mae, custom_predictions = custom_model.calculate_mae(test_data)\n",
    "\n",
    "      y_pred = np.expm1(custom_predictions)\n",
    "      y_true = np.expm1(y_test)\n",
    "\n",
    "      custom_mae = mean_absolute_error(y_true, y_pred)\n",
    "      mae_values.append(custom_mae)\n",
    "\n",
    "  average_mae = np.mean(mae_values)\n",
    "  mae_avg.append(average_mae)\n",
    "\n",
    "print(f\"MAE: {np.mean(mae_avg)} +/- {np.std(mae_avg)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XUxZYKBu43u2"
   },
   "source": [
    "## **9. GitHub Link**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLutq8Oc5AVp"
   },
   "source": [
    "https://github.com/adang66/Print-Hello-World-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFLvoEJiieCM"
   },
   "source": [
    "## **10. References**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v5YZThuVilur"
   },
   "source": [
    "* Cortez, P. and Morais, A.D.J.R., 2007. A data mining approach to predict forest fires using meteorological data.\n",
    "\n",
    "* Breiman, L., 2017. Classification and regression trees. Routledge.\n",
    "\n",
    "* [Scikit-learn: Tree Algorithms (CART)](https://scikit-learn.org/1.5/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
